{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mod√®le Logistic Regression (avec r√©duction de dimension avant PCA ou SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 1Ô∏è‚É£ Charger les datasets\n",
    "train_df = pd.read_csv(\"application_train.csv\")\n",
    "test_df = pd.read_csv(\"application_test.csv\")\n",
    "\n",
    "# 2Ô∏è‚É£ S√©parer features et cible\n",
    "X_train = train_df.drop(columns=[\"TARGET\", \"SK_ID_CURR\"])\n",
    "y_train = train_df[\"TARGET\"]\n",
    "X_test = test_df.drop(columns=[\"SK_ID_CURR\"])\n",
    "\n",
    "# 3Ô∏è‚É£ Standardisation\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 4Ô∏è‚É£ PCA (r√©duction de dimension)\n",
    "pca = PCA(n_components=0.95)  # Garde 95% de la variance\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# 5Ô∏è‚É£ SVD (alternative pour sparse data)\n",
    "svd = TruncatedSVD(n_components=50)  # On r√©duit √† 50 dimensions\n",
    "X_train_svd = svd.fit_transform(X_train_scaled)\n",
    "X_test_svd = svd.transform(X_test_scaled)\n",
    "\n",
    "# 6Ô∏è‚É£ Entra√Ænement Logistic Regression (Baseline, PCA, SVD)\n",
    "models = {\n",
    "    \"Baseline (Sans PCA/SVD)\": X_train_scaled,\n",
    "    \"Avec PCA\": X_train_pca,\n",
    "    \"Avec SVD\": X_train_svd\n",
    "}\n",
    "\n",
    "submissions = {}\n",
    "\n",
    "for name, X_train_model in models.items():\n",
    "    X_test_model = X_test_pca if \"PCA\" in name else (X_test_svd if \"SVD\" in name else X_test_scaled)\n",
    "    \n",
    "    model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    model.fit(X_train_model, y_train)\n",
    "    \n",
    "    y_pred_proba = model.predict_proba(X_test_model)[:, 1]\n",
    "    \n",
    "    # Stocker les pr√©dictions\n",
    "    submissions[name] = pd.DataFrame({\"SK_ID_CURR\": test_df[\"SK_ID_CURR\"], \"TARGET\": y_pred_proba})\n",
    "    submissions[name].to_csv(f\"submission_{name.replace(' ', '_')}.csv\", index=False)\n",
    "    \n",
    "    print(f\"{name} - Pr√©dictions sauvegard√©es.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mod√®le Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 1Ô∏è‚É£ Charger les datasets\n",
    "train_df = pd.read_csv(\"application_train.csv\")\n",
    "test_df = pd.read_csv(\"application_test.csv\")\n",
    "\n",
    "# 2Ô∏è‚É£ S√©parer features et cible\n",
    "X_train = train_df.drop(columns=[\"TARGET\", \"SK_ID_CURR\"])\n",
    "y_train = train_df[\"TARGET\"]\n",
    "X_test = test_df.drop(columns=[\"SK_ID_CURR\"])\n",
    "\n",
    "# 3Ô∏è‚É£ Entra√Ænement du mod√®le Random Forest\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,  # Nombre d'arbres (√† ajuster)\n",
    "    max_depth=10,  # Profondeur max (√©vite overfitting)\n",
    "    random_state=42,\n",
    "    n_jobs=-1,  # Utilise tous les CPU pour acc√©l√©rer\n",
    "    class_weight=\"balanced\"  # G√®re le d√©s√©quilibre des classes\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# 4Ô∏è‚É£ Pr√©dictions sur application_test\n",
    "y_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 5Ô∏è‚É£ G√©n√©ration du fichier de soumission\n",
    "submission_rf = pd.DataFrame({\"SK_ID_CURR\": test_df[\"SK_ID_CURR\"], \"TARGET\": y_pred_proba})\n",
    "submission_rf.to_csv(\"submission_RandomForest.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Random Forest - Pr√©dictions sauvegard√©es.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mod√®le XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# 1Ô∏è‚É£ Charger les datasets\n",
    "train_df = pd.read_csv(\"application_train.csv\")\n",
    "test_df = pd.read_csv(\"application_test.csv\")\n",
    "\n",
    "# 2Ô∏è‚É£ S√©parer features et cible\n",
    "X_train = train_df.drop(columns=[\"TARGET\", \"SK_ID_CURR\"])\n",
    "y_train = train_df[\"TARGET\"]\n",
    "X_test = test_df.drop(columns=[\"SK_ID_CURR\"])\n",
    "\n",
    "# 3Ô∏è‚É£ Configuration de XGBoost\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=500,  # Nombre d'arbres\n",
    "    learning_rate=0.05,  # Vitesse d'apprentissage\n",
    "    max_depth=6,  # Profondeur des arbres\n",
    "    subsample=0.8,  # √âchantillonnage des donn√©es pour √©viter l'overfitting\n",
    "    colsample_bytree=0.8,  # √âchantillonnage des features pour chaque arbre\n",
    "    objective=\"binary:logistic\",  # Probl√®me de classification binaire\n",
    "    eval_metric=\"auc\",  # Optimisation du ROC-AUC\n",
    "    use_label_encoder=False,\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Utilise tous les CPU\n",
    ")\n",
    "\n",
    "# 4Ô∏è‚É£ Entra√Ænement du mod√®le\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# 5Ô∏è‚É£ Pr√©dictions sur application_test\n",
    "y_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 6Ô∏è‚É£ G√©n√©ration du fichier de soumission\n",
    "submission_xgb = pd.DataFrame({\"SK_ID_CURR\": test_df[\"SK_ID_CURR\"], \"TARGET\": y_pred_proba})\n",
    "submission_xgb.to_csv(\"submission_XGBoost.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ XGBoost - Pr√©dictions sauvegard√©es.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mod√®le FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 1Ô∏è‚É£ Charger les datasets\n",
    "train_df = pd.read_csv(\"application_train.csv\")\n",
    "test_df = pd.read_csv(\"application_test.csv\")\n",
    "\n",
    "# 2Ô∏è‚É£ S√©parer features et labels\n",
    "X_train = train_df.drop(columns=[\"TARGET\", \"SK_ID_CURR\"])\n",
    "y_train = train_df[\"TARGET\"].values\n",
    "X_test = test_df.drop(columns=[\"SK_ID_CURR\"])\n",
    "\n",
    "# 3Ô∏è‚É£ Normalisation (important pour les r√©seaux de neurones)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 4Ô∏è‚É£ Conversion en Tensors PyTorch\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)  # Reshape pour BCE Loss\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "# 5Ô∏è‚É£ Cr√©ation du DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "\n",
    "# 6Ô∏è‚É£ D√©finition du MLP (FNN)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),  # Evite l‚Äôoverfitting\n",
    "            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()  # Probabilit√© entre 0 et 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 7Ô∏è‚É£ Initialisation du mod√®le\n",
    "input_dim = X_train.shape[1]\n",
    "model = MLP(input_dim)\n",
    "\n",
    "# 8Ô∏è‚É£ D√©finition de la Loss et de l‚ÄôOptimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 9Ô∏è‚É£ Entra√Ænement du mod√®le\n",
    "num_epochs = 20\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# üîü Pr√©dictions sur les donn√©es de test\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_proba = model(X_test_tensor.to(device)).cpu().numpy()\n",
    "\n",
    "# üî• Sauvegarde du fichier de soumission\n",
    "submission_nn = pd.DataFrame({\"SK_ID_CURR\": test_df[\"SK_ID_CURR\"], \"TARGET\": y_pred_proba.flatten()})\n",
    "submission_nn.to_csv(\"submission_NN.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ R√©seau de Neurones - Pr√©dictions sauvegard√©es !\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
